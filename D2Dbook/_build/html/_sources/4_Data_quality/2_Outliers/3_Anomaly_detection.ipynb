{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Anomaly detection\n",
    "- In contrast to statistical outliers, anomaly detection is more often concerned with local deviations than deviations from a common distribution.\n",
    "- We will cover two density based methods and one tree based method.\n",
    "- Density based methods are highly dependent on scaling.\n",
    "    - The amount and type of scaling is problem dependent and can be considered part of the tuning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## DBSCAN \n",
    "- Density-Based Spatial Clustering of Applications with Noise.\n",
    "- No assumptions about distributions.\n",
    "- Definitions:\n",
    "    - 'Core point' if >= MinPts within radius $\\epsilon$.\n",
    "    - 'Border point' if < MinPts within radius $\\epsilon$, but within radius of a 'core point'.\n",
    "    - Else a 'Noise points'.\n",
    "- Clustering:\n",
    "    - Cluster 'core points' that lay within each others radii.\n",
    "    - Assign 'border points' to their respective 'core point' clusters.\n",
    "- Our main interest is in detecting 'noise points', i.e., outliers.\n",
    "    - Also, small clusters may be indicative of series of outliers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### DBSCAN in 2D\n",
    "- Illustration of the concept in 2D from [Wikimedia CC-SA 3.0 by Chire](https://commons.wikimedia.org/wiki/File:DBSCAN-Illustration.svg)  \n",
    "<img src=\"https://github.com/khliland/IND320/blob/main/D2Dbook/images/DBSCAN.png?raw=TRUE\" width=\"300px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### DBSCAN in 1D\n",
    "- To avoid a pure vertical clustering in the charts, we need to use the observation/time dimension actively.\n",
    "- The horizontal spacing between points in the chart will be an extra parameter to tune.\n",
    "- More than one variable can be included in DBSCAN, but these must be matched by some form of scaling, e.g., standardisation.\n",
    "- DBSCAN does not care about drift in mean values, only local density (pro and con)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Random normal data\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "np.random.seed(1)\n",
    "N = 1000\n",
    "data = np.random.normal(0, 1, N)\n",
    "plt.figure(figsize=(10,3))\n",
    "plt.plot(data, 'o')\n",
    "plt.ylim(-4.5, 4.5)\n",
    "plt.xlim(0, N)\n",
    "plt.axhline(0, color='black', linestyle='--')\n",
    "plt.ylabel('Values')\n",
    "plt.xlabel('Observations')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Import DBSCAN from sklearn\n",
    "from sklearn.cluster import DBSCAN\n",
    "\n",
    "# Reshape the data to a column vector together with an index column\n",
    "step_size = 0.02\n",
    "data2D = np.array([data, np.linspace(0, N*step_size, N)]).T\n",
    "\n",
    "# Initialize and fit the DBscan model\n",
    "db = DBSCAN(eps=0.5, min_samples=3, metric='euclidean')\n",
    "db.fit(data2D)\n",
    "\n",
    "# Obtain the predicted labels and calculate number of clusters\n",
    "pred_labels = db.labels_\n",
    "# -1 is an outlier, >=0 is a cluster\n",
    "\n",
    "# Count number of samples in each cluster\n",
    "counts = np.bincount(pred_labels+1)\n",
    "counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Show the cluster labels\n",
    "from matplotlib.lines import Line2D\n",
    "plt.figure(figsize=(10,3))\n",
    "plt.plot(data2D[:,1], data2D[:,0], 'o')\n",
    "plt.ylim(-4.5, 4.5)\n",
    "plt.xlim(0, max(data2D[:,1]))\n",
    "plt.axhline(0, color='black', linestyle='--')\n",
    "plt.ylabel('Values')\n",
    "plt.xlabel('Scaled observations')\n",
    "# Plot special samples in red and orange\n",
    "for i in range(len(data)):\n",
    "    if pred_labels[i] == -1:\n",
    "        plt.plot(data2D[i,1], data2D[i,0], 'o', color='red')\n",
    "    if pred_labels[i] > 0:\n",
    "        plt.plot(data2D[i,1], data2D[i,0], 'o', color='orange')\n",
    "legend_elements = [Line2D([0], [0], marker='o', color='red', label='outlier', linestyle='None'),\n",
    "                   Line2D([0], [0], marker='o', color='orange', label='cluster', linestyle='None')]\n",
    "plt.legend(handles=legend_elements, bbox_to_anchor=(1.05, 1), loc=2)\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Convert the above plot to Plotly format\n",
    "import plotly.graph_objects as go\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(x=data2D[:,1], y=data2D[:,0], mode='markers', marker=dict(color='blue'), name='normal'))\n",
    "fig.add_trace(go.Scatter(x=data2D[:,1][pred_labels==-1], y=data2D[pred_labels==-1,0], mode='markers', marker=dict(color='red'), name='outlier'))\n",
    "fig.add_trace(go.Scatter(x=data2D[:,1][pred_labels>0], y=data2D[pred_labels>0,0], mode='markers', marker=dict(color='orange'), name='cluster'))\n",
    "fig.update_layout(title='DBSCAN Anomaly Detection', xaxis_title='Scaled observations', yaxis_title='Values')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Exercise\n",
    "- Look at the DEWP measurements in the Beijing pollution data (2000 timepoints).\n",
    "- Try to tune a DBSCAN on the data.\n",
    "- Does it work as advertised?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Local Outlier Factor - LOF\n",
    "- LOF is a method that tries to determine if an observation is an outlier based on the density of observations around itself compared to the density of observations around its k nearest neighbours.\n",
    "- The general assumption is that $LOF_k(A) < 1$ is an outlier, but this is problem dependent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Definitions for LOF\n",
    "For an observation A and a neighbour B:  \n",
    "- k-distance(A) = distance(A, k-th nearest neighbour)\n",
    "- k-neighbourhood:  \n",
    "  $N_k(A) = \\{\\text{B} | \\text{distance(A, B)} \\leq \\text{k-distance(A)}\\}$,  \n",
    "  i.e., all observations no farther away than the k-th nearest point.\n",
    "- Reachability distance:  \n",
    "  $RD_k(\\text{A, B}) = max\\{\\text{k-distance(B)}, \\text{distance(A, B)}\\}$,  \n",
    "  i.e., the maximum of observation B's k-distance and the actual distance between A and B.  \n",
    "  This is maybe the least intuitive definition here as it is called a distance but is based on density and thus is unsymmetric if you change around A and B. In the figure below, $RD_3(\\text{A, B}) < RD_3(\\text{A, C})$ even though A is closer to B than to C.  \n",
    "  <img src=\"https://github.com/khliland/IND320/blob/main/D2Dbook/images/Outliers_LOF_RD.png?raw=TRUE\" width=\"300px\">  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- Local reachability density:  \n",
    "  $lrd_k(A) = 1 / \\frac{\\sum_{B \\in N_k(A)} RD_k(A, B)}{|N_k(A)|}$,  \n",
    "  i.e., the inverse of the average reachability distance of A from its neighbours.\n",
    "- Local Outlier Factor:  \n",
    "  $LOF_k(A) = \\frac{\\sum_{B \\in N_k(A)}\\frac{lrd_k(B)}{lrd_k(A)}}{|N_k(A)|} = \\frac{\\sum_{B \\in N_k(A)}lrd_k(B)}{|N_k(A)| lrd_k(A)}$,  \n",
    "  i.e., the average local reachability density of the neighbors divided by the object's own local reachability density."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### [LOF in scikit-learn](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.LocalOutlierFactor.html)\n",
    "- As nearest neighbour searches can be time and memory intensive, the implementation lets the user choose between NN algorithms.\n",
    "- The type of distance can be chosen.\n",
    "- A \"contamination\" parameter is used to score samples by scikit-learn. This is the proportion of outliers, which can be set to \"auto\" for automatic estimation. Otherwise the proportion is used to find the threshold of outlyingness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Import LOF from sklearn\n",
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "\n",
    "# Use the same data as above but replace DBSCAN with LOF\n",
    "lof = LocalOutlierFactor(n_neighbors=20, contamination='auto') # 0.01)\n",
    "pred_labels = lof.fit_predict(data2D)\n",
    "counts = np.bincount(pred_labels+1)\n",
    "print(counts)\n",
    "print(counts[0]/sum(counts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Plot the LOF values\n",
    "lof_pos = -lof.negative_outlier_factor_\n",
    "plt.figure(figsize=(7,3))\n",
    "plt.hist(lof_pos, bins=50)\n",
    "plt.xlabel('LOF values')\n",
    "plt.ylabel('Number of samples')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Using default cutoff value of LOF = 1\n",
    "lof_labels = 1-(lof_pos > 1)*2\n",
    "np.bincount(lof_labels+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Plot the results in the same way as above\n",
    "preds = pred_labels.copy()\n",
    "#preds = lof_labels\n",
    "plt.figure(figsize=(10,3))\n",
    "plt.plot(data2D[:,1], data2D[:,0], 'o')\n",
    "plt.ylim(-4.5, 4.5)\n",
    "plt.xlim(0, max(data2D[:,1]))\n",
    "plt.axhline(0, color='black', linestyle='--')\n",
    "plt.ylabel('Values')\n",
    "plt.xlabel('Scaled observations')\n",
    "for i in range(len(data)):\n",
    "    if preds[i] == -1:\n",
    "        plt.plot(data2D[i,1], data2D[i,0], 'o', color='red')\n",
    "legend_elements = [Line2D([0], [0], marker='o', color='red', label='outlier', linestyle='None')]\n",
    "plt.legend(handles=legend_elements, bbox_to_anchor=(1.05, 1), loc=2)\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Isolation Forest\n",
    "- Isolation Forests use binary trees with random splits along variables/features to isolate outlying samles.\n",
    "- Basically, outlying observations are easier to split from inlying observation, thus having a shorter path through the tree before becomming a leaf node.\n",
    "- In contrast to DBSCAN and LOF, no density estimation is performed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### [Isolation Forest](https://en.wikipedia.org/wiki/Isolation_forest) algorithm\n",
    "- An Isolation Tree (iTree) is built on a subset of the samples as follows:\n",
    "    1. Randomly choose a feature.\n",
    "    2. Randomly split the (remaining) data along the feature.\n",
    "    3. If no more splits are possible (all observations are leafs (alone) or all samples are equal in the end nodes): terminate,  \n",
    "      otherwise, repeat from 1.\n",
    "- The forest is created by building many iTrees (100 is default in [scikit-learn](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.IsolationForest.html)).\n",
    "- Samples are deemed outliers if they on average become leaf nodes after few splits (low path-lengths in the trees).\n",
    "- In scikit-learn the cut-off for being an outlier is estimated automatically, but can be changed using the \"contamination\" parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Import IsolationForest from sklearn\n",
    "from sklearn.ensemble import IsolationForest\n",
    "\n",
    "# Use the same data as above but replace DBSCAN with IsolationForest\n",
    "iso = IsolationForest(n_estimators=100, contamination=0.05)#'auto')\n",
    "pred_labels_if = iso.fit_predict(data2D)\n",
    "counts = np.bincount(pred_labels_if+1)\n",
    "print(counts)\n",
    "print(counts[0]/sum(counts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Plot the IF values\n",
    "if_vals = iso.decision_function(data2D)\n",
    "plt.figure(figsize=(7,3))\n",
    "plt.hist(if_vals, bins=50)\n",
    "plt.xlabel('IF values')\n",
    "plt.ylabel('Number of samples')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if_labels = 1-(if_vals < -0.15)*2\n",
    "np.bincount(if_labels+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Plot the results in the same way as above\n",
    "preds_if = pred_labels_if.copy()\n",
    "#preds_if = if_labels\n",
    "plt.figure(figsize=(10,3))\n",
    "plt.plot(data2D[:,1], data2D[:,0], 'o')\n",
    "plt.ylim(-4.5, 4.5)\n",
    "plt.xlim(0, max(data2D[:,1]))\n",
    "plt.axhline(0, color='black', linestyle='--')\n",
    "plt.ylabel('Values')\n",
    "plt.xlabel('Scaled observations')\n",
    "for i in range(len(data)):\n",
    "    if preds_if[i] == -1:\n",
    "        plt.plot(data2D[i,1], data2D[i,0], 'o', color='red')\n",
    "legend_elements = [Line2D([0], [0], marker='o', color='red', label='outlier', linestyle='None')]\n",
    "plt.legend(handles=legend_elements, bbox_to_anchor=(1.05, 1), loc=2)\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Exercise\n",
    "- Compare the three mentioned anomaly detection methods (DBSCAN, LOF and Isolation Forest) with regard to the samples seen as outliers.\n",
    "    - Fix the parameters at _min\\_samples_ = 3, _n\\_neighbors_ = 20 and _n\\_estimators_ = 100 and use _eps_ and _contamination_ to force 1, 2, ..., 10 outliers detected (if possible).\n",
    "    - Are the three sets of 10 samples equal.\n",
    "    - Do the samples appear in the same order for the different techniques?  \n",
    "      Spearman correlation is one way of putting this into numbers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "```{seealso} \n",
    ":class: tip\n",
    "\n",
    "## Resources\n",
    "- [Wikipedia: Local Outlier Factor](https://en.wikipedia.org/wiki/Local_outlier_factor)\n",
    "- [LOF in scikit-learn](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.LocalOutlierFactor.html)\n",
    "- [Isolation Forest](https://en.wikipedia.org/wiki/Isolation_forest)\n",
    "- [scikit-learn: Isolation Forest](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.IsolationForest.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "# Dummy cell to ensure Plotly graphics are shown\n",
    "import plotly.graph_objects as go\n",
    "f = go.FigureWidget([go.Scatter(x=[1,1], y=[1,1], mode='markers')])"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
