{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Outlier statistics\n",
    "- Determining if a sample or timepoint is a statistical outlier is often a two-step process:\n",
    "    1. Estimate a distribution assumed to be normal operating conditions.\n",
    "    2. Check if new samples are significant outliers from this distribution.\n",
    "- (Multivariate) [Statistical Process Control](https://en.wikipedia.org/wiki/Statistical_process_control), part of [$6\\sigma$](https://en.wikipedia.org/wiki/Six_Sigma) process improvement, has a wide range of methods for this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Statistical Process Control - SPC\n",
    "- The simplest form handles a single variable.\n",
    "- A normal distribution is assumed, but some deviation is tolerated.\n",
    "- It also assumes a constant/stationary process and no shifts/trends in distributions.\n",
    "- Any value outside +/- 3 standard deviations (SD) of the mean is assumed to be an outlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Plot the normal probability curve from -4 to 4 with mean 0 and standard deviation 1\n",
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(5,3))\n",
    "x = np.linspace(-4, 4, 100)\n",
    "y = stats.norm.pdf(x, 0, 1)\n",
    "plt.plot(x, y, color='black')\n",
    "plt.xlabel('Standard deviations')\n",
    "plt.ylabel('Probability density')\n",
    "plt.title('Standard normal probability curve')\n",
    "# Shade the area under the curve to the left of -3 and right of 3\n",
    "px = np.linspace(-4, -3, 100)\n",
    "py = stats.norm.pdf(px, 0, 1)\n",
    "plt.fill_between(px, py, color='red')\n",
    "px = np.linspace(3, 4, 100)\n",
    "py = stats.norm.pdf(px, 0, 1)\n",
    "plt.fill_between(px, py, color='red')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Calculate the probability of being outside +/- 3 SD in a normal distribution, i.e. P(|X| > 3).\n",
    "# Format the result as a percentage rounded to two decimal places.\n",
    "import scipy.stats as stats\n",
    "prob = stats.norm.cdf(-3)*2\n",
    "print('Probability of being outside +/- 3 SD in a normal distribution: {:.2%}'.format(prob))\n",
    "# Rewrite this as the proportion of values, i.e., 1 in n, that are outside +/- 3 SD in a normal distribution.\n",
    "print('Proportion of values that are outside +/- 3 SD in a normal distribution: 1 in {:.0f}'.format(1/prob))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Control charts\n",
    "- A common way of assessing outliers is to plot the series together with lower and upper boundaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Plot random normal data with 1000 values with a seed of 1\n",
    "import numpy as np\n",
    "np.random.seed(1)\n",
    "data = np.random.normal(0, 1, 1000)\n",
    "plt.figure(figsize=(10,3))\n",
    "plt.plot(data, 'o')\n",
    "plt.ylim(-4.5, 4.5)\n",
    "plt.xlim(0, 1000)\n",
    "plt.axhline(0, color='black', linestyle='--')\n",
    "plt.axhline(3, color='red')\n",
    "plt.axhline(-3, color='red')\n",
    "plt.ylabel('Standard deviations')\n",
    "plt.xlabel('Observations')\n",
    "# Plot all samples that are outside +/- 3 SD in a normal distribution in red\n",
    "outliers = data[np.abs(data) > 3]\n",
    "plt.plot(np.where(np.abs(data) > 3)[0], outliers, 'o', color='red')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Histograms\n",
    "- Histograms show you a cummulative view over a set of observations.\n",
    "- If the observed distribution is very different from a bell curve (normal distribution), the +/-3 SD will not have the expected coverage.\n",
    "    - Associated P-values will be wrong."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Plot the same data as a histogram\n",
    "plt.figure(figsize=(5,3))\n",
    "plt.hist(data, bins=50)\n",
    "plt.ylabel('Frequency')\n",
    "plt.xlabel('Standard deviations')\n",
    "plt.title('Histogram of random normal data')\n",
    "plt.axvline(3, color='red')\n",
    "plt.axvline(-3, color='red')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Calculate the probability of observing a value more extreme \n",
    "# than the maximum absolute value in the data set at random\n",
    "stats.norm.sf(np.max(np.abs(data)))*2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Quantile plots\n",
    "- A quantile plot plots expected quantiles of a distributon against observed quantiles from data.\n",
    "- The basic distribution is the normal probability distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# A quantile plot can also help to identify outliers and deviations from normality\n",
    "plt.figure(figsize=(4,3))\n",
    "stats.probplot(data, dist='norm', plot=plt)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Robust statistics\n",
    "- Instead of mean and standard deviation, one can use more robust calculations.\n",
    "- _Trimmed mean_ means first removing a proportion, e.g., 5\\% of the most extreme observations before calculating the mean.\n",
    "    - Robust against outliers.\n",
    "- [_Median absolute deviation_](https://en.wikipedia.org/wiki/Median_absolute_deviation) is the median of the absolute deviations from the median value.\n",
    "    - Robust against non-normality.\n",
    "  \n",
    "$$MAD = median(|X_i - \\tilde{X}|), ~~~ \\tilde{X} = median(X)$$\n",
    "  \n",
    "- Relation to standard deviation: $\\hat{\\sigma} = k \\cdot MAD$.\n",
    "- For normal data $k \\approx 1.4826$, see Wikipedia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Trimmed mean\n",
    "m1 = np.mean(data)\n",
    "m2 = stats.trim_mean(data, 0.05)\n",
    "print('Mean: {:.3f}'.format(m1))\n",
    "print('Trimmed mean: {:.3f}'.format(m2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Median absolute deviation\n",
    "s1 = np.std(data)\n",
    "s2 = stats.median_abs_deviation(data)\n",
    "print('Standard deviation: {:.3f}'.format(s1))\n",
    "print('Median absolute deviation: {:.3f}'.format(s2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# SD vs MAD\n",
    "print('Adjusted MAD: {:.3f}'.format(s2 * 1.4826))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Heuristics\n",
    "- As seen above, to be flagged in the base case will happen by accident 1 in 370 cases.\n",
    "- The probability quickly shrinks if additional requirements are added, e.g., 3 cases in a row outside +/- 3 std.\n",
    "    - With multiple consecutive outliers, one can also use fewer standard deviations.\n",
    "- If data are assumed to be iid (independent and identically distributed), another heuristic can be to check if concecutive samples are too similar (and possibly non-centred).\n",
    "    - This can indicate a bias in the series, e.g., caused by a manufacturing step caught in an error condition.\n",
    "- A shift in mean value can also be indicative of errors or unwanted changes; checkable using rolling means or similar with an appropriate window size.\n",
    "    - A whole field of SPC uses Exponentially Weighted Moving Averages to detect outliers and shifts in distributions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Assuming independent samples, calculate the probability of observing\n",
    "# three values in a row above 2 SD in a normal distribution.\n",
    "prob3 = (1-stats.norm.cdf(2))**3\n",
    "print('Probability: {:.4%}'.format(prob3))\n",
    "print('Proportion: 1 in {:.0f}'.format(1/prob3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Create a function that checks if n consecutive samples are above k SD or below k SD\n",
    "def check_consecutive(data, k, n, mu, SD):\n",
    "    index = []\n",
    "    dataUp = (data-mu) > k*SD\n",
    "    dataDown = (data-mu) < -k*SD\n",
    "    for i in range(len(data)-n+1):\n",
    "        if np.all(dataUp[i:i+n]) or np.all(dataDown[i:i+n]):\n",
    "            index.append(i)\n",
    "    return index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Apply hueristic function and plot\n",
    "k = 2\n",
    "n = 2\n",
    "mu = 0\n",
    "SD = 1\n",
    "index = check_consecutive(data, k, n, mu, SD)\n",
    "\n",
    "plt.figure(figsize=(10,3))\n",
    "plt.plot(data, 'o')\n",
    "plt.ylim(-4.5, 4.5)\n",
    "plt.xlim(0, 1000)\n",
    "plt.axhline(0, color='black', linestyle='--')\n",
    "plt.axhline(k*SD, color='red')\n",
    "plt.axhline(-k*SD, color='red')\n",
    "plt.ylabel('Standard deviations')\n",
    "plt.xlabel('Observations')\n",
    "# Plot all samples that are caught by the heuristic function\n",
    "for i in index:\n",
    "    plt.plot(range(i,i+n), data[range(i,i+n)], 'o', color='yellow')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Exercise\n",
    "- Import the data called _bananas.csv_.\n",
    "- Assume the first 500 samples are \"in control\" and calculate their _mean_ and _standard deviation_.\n",
    "- Plot the whole series and indicate outliers using SPC and heuristics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Multivariate series\n",
    "- Though each variable can be handled separately with SPC, it is often more interesting to assess the combined effect.\n",
    "- Smaller deviations in single variables may be detected if these occur in relation to other variables and their variation.\n",
    "- Instead of the normal distribution, we use [Mahalanobis distance](https://en.wikipedia.org/wiki/Mahalanobis_distance) and the associated [Hotelling's $T^2$](https://en.wikipedia.org/wiki/Hotelling%27s_T-squared_distribution), a multivariate generalisation of the student t-distribution. $t^2$ is defined as:  \n",
    "  \n",
    "$$t^2 = (\\bar{x}-\\mu) \\hat{\\Sigma}_{\\bar{x}}^{-1} (\\bar{x}-\\mu)^T$$\n",
    "  \n",
    "- In practice we estimate $\\bar{x}$ and $\\hat{\\Sigma}_{\\bar{x}}$ from \"in control\" data and look at single observations for $\\mu$.\n",
    "  \n",
    "$$\\frac{n-p}{p(n-1)}t^2 \\sim F_{p,n-p}$$\n",
    "  \n",
    "where $n$ is the number of samples in the training data and $p$ is the number of variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Read the Beijing pollution data\n",
    "import pandas as pd\n",
    "pollution = pd.read_csv(\"../../data/pollution.csv\", header=0, index_col=0)\n",
    "# Extract the first 2000 samples of DEWP, TEMP and PRES\n",
    "pdata = pollution[['DEWP', 'TEMP', 'PRES']].iloc[:2000]\n",
    "\n",
    "# Plot the three variables as separate subplots above each other\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.subplot(311)\n",
    "plt.plot(pdata['DEWP'], '-')\n",
    "plt.ylabel('DEWP')\n",
    "plt.subplot(312)\n",
    "plt.plot(pdata['TEMP'], '-')\n",
    "plt.ylabel('TEMP')\n",
    "plt.subplot(313)\n",
    "plt.plot(pdata['PRES'], '-')\n",
    "plt.ylabel('PRES')\n",
    "plt.xlabel('Observation')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We use the first three years as the \"in control\" region and estimate mean and covariance from it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Calculate the mean vector and covariance matrix of the three variables for the first 3 years\n",
    "import numpy as np\n",
    "mean = np.mean(pdata.values[:365*3,:], axis=0)\n",
    "cov = np.cov(pdata.values[:365*3,:], rowvar=False)\n",
    "print(mean)\n",
    "print(cov)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Calculate the Hotelling's T^2 statistic for each observation given mu and cov\n",
    "def Hotellings_T2(X, mean, cov, n, alpha = 0.01):\n",
    "    T2 = np.sum(((X-mean) @ np.linalg.inv(cov)) * (X-mean), axis=-1)\n",
    "    p = len(mean)\n",
    "    F = (n-p)/(p*(n-1))*T2\n",
    "    P = stats.f.sf(F, p, n-p)\n",
    "    # Critical value\n",
    "    c = stats.f.isf(alpha, p, n-p)*p*(n-1)/(n-p)\n",
    "    return (T2, F, P, c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Apply Hotellings_T2 and plot\n",
    "alpha = 0.0027\n",
    "T2, F, P, c = Hotellings_T2(pdata.values, mean, cov, 365*3, alpha)\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.subplot(211)\n",
    "plt.plot(T2)\n",
    "plt.grid()\n",
    "plt.axhline(c, color=\"red\")\n",
    "plt.axvline(365*3, color=\"orange\")\n",
    "plt.ylabel(\"T2\")\n",
    "plt.subplot(212)\n",
    "# Plot P on a logarithmic scale\n",
    "plt.plot(P)\n",
    "plt.yscale(\"log\")\n",
    "plt.grid()\n",
    "plt.axhline(alpha, color=\"red\")\n",
    "plt.axvline(365*3, color=\"orange\")\n",
    "plt.ylabel(\"P-value\")\n",
    "plt.xlabel(\"Observation\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Finally, we observe which observations are marked as outlying in the original series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Plot the three variables as separate subplots above each other, marking the outlying regions in red\n",
    "x = np.arange(2000)+0.0\n",
    "x[P>alpha] = np.nan\n",
    "\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.subplot(311)\n",
    "plt.plot(pdata['DEWP'], '-')\n",
    "plt.plot(x, pdata['DEWP'], '-', color='red')\n",
    "plt.ylabel('DEWP')\n",
    "plt.subplot(312)\n",
    "plt.plot(pdata['TEMP'], '-')\n",
    "plt.plot(x, pdata['TEMP'], '-', color='red')\n",
    "plt.ylabel('TEMP')\n",
    "plt.subplot(313)\n",
    "plt.plot(pdata['PRES'], '-')\n",
    "plt.plot(x, pdata['PRES'], '-', color='red')\n",
    "plt.ylabel('PRES')\n",
    "plt.xlabel('Observation')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Robust multivariate statistics\n",
    "- Also multivariate data can need robust statistics because of non-normality or outliers.\n",
    "- One method for robust estimation of mean and standard deviation is called Minimum Covariance Determinant (MCD).\n",
    "- MCD uses a subset of samples that minimises the determinant of the covariance matrix.\n",
    "- [An iterative method that searches for the MCD](https://www.researchgate.net/publication/2298225_A_Fast_Algorithm_for_the_Minimum_Covariance_Determinant_Estimator) is implemented in scikit-learn's [MinCovDet](https://scikit-learn.org/stable/modules/generated/sklearn.covariance.MinCovDet.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Import Minimum Covariance Determinant estimator from scikit-learn\n",
    "from sklearn.covariance import MinCovDet\n",
    "# Apply the Minimum Covariance Determinant estimator to data.values[:365*3,:] and plot\n",
    "some_data = np.random.multivariate_normal(mean, cov, 10)\n",
    "mcd = MinCovDet(random_state=1).fit(some_data)\n",
    "mcd_mean = mcd.location_\n",
    "mcd_cov = mcd.covariance_\n",
    "mcd2 = MinCovDet(random_state=3).fit(pdata.values)\n",
    "mcd2_mean = mcd2.location_\n",
    "mcd2_cov = mcd2.covariance_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(mcd_mean)\n",
    "print(mcd_cov)\n",
    "print(mcd2_mean)\n",
    "print(mcd2_cov)\n",
    "print(mean)\n",
    "print(cov)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Exercise\n",
    "- Repeat the calculations of mean, covariance, Hotelling's $T^2$, etc. for the Beijing pollution data.\n",
    "- Exchange ordinary statistics with the MCD alternative."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Time resolution\n",
    "- The (M)SPC examples have assumed a constant mean and standard deviation.\n",
    "- It may also be interesting to look at det deviation from smoothed data to look for local outliers.\n",
    "    - This corresponds to a high-pass filter for FFT.\n",
    "- In the rich field of (M)SPC there are also other methods tailored for finding shifts in trends."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from scipy.fft import dct, idct\n",
    "fourier_signal = dct(pdata[\"TEMP\"].values)\n",
    "\n",
    "# Plot the Fourier transform\n",
    "plt.figure(figsize=(7,2))\n",
    "plt.plot(np.abs(fourier_signal))\n",
    "plt.xlabel('Frequency')\n",
    "plt.ylabel('Fourier Transform')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Filter the Fourier transform by setting all frequencies below a threshold to zero\n",
    "W = np.arange(0, 2000) # Frequency axis\n",
    "filtered_fourier_signal = fourier_signal.copy()\n",
    "filtered_fourier_signal[(W<30)] = 0\n",
    "cut_signal = idct(filtered_fourier_signal)\n",
    "\n",
    "# Plot the filtered signal\n",
    "plt.figure(figsize=(7,4))\n",
    "plt.subplot(211)\n",
    "plt.plot(pdata[\"TEMP\"].values)\n",
    "plt.plot(pdata[\"TEMP\"].values - cut_signal)\n",
    "plt.ylabel('Signal and trend')\n",
    "plt.subplot(212)\n",
    "plt.plot(cut_signal)\n",
    "plt.ylabel('Local variation')\n",
    "plt.xlabel('Observation')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "```{seealso} \n",
    ":class: tip\n",
    "\n",
    "## Resources\n",
    "- [Wikipedia: Statistical Process Control](https://en.wikipedia.org/wiki/Statistical_process_control)\n",
    "- [Wikipedia: $6\\sigma$](https://en.wikipedia.org/wiki/Six_Sigma)\n",
    "- [Wikipedia: Median absolute deviation](https://en.wikipedia.org/wiki/Median_absolute_deviation)\n",
    "- [Wikipedia: Mahalanobis distance](https://en.wikipedia.org/wiki/Mahalanobis_distance) \n",
    "- [Wikipedia: Hotelling's $T^2$](https://en.wikipedia.org/wiki/Hotelling%27s_T-squared_distribution)\n",
    "- [An iterative method that searches for the MCD](https://www.researchgate.net/publication/2298225_A_Fast_Algorithm_for_the_Minimum_Covariance_Determinant_Estimator)\n",
    "- [scikit-learn: MinCovDet](https://scikit-learn.org/stable/modules/generated/sklearn.covariance.MinCovDet.html)"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
