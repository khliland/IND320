{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Spark\n",
    "- A fast and general compute engine (originally for [Hadoop](https://hadoop.apache.org/) data).\n",
    "    - Often paired with Hadoop for its distributed filesystem (HDFS), cluster resource management and parallel processing.\n",
    "- Spark provides a simple and expressive programming model that supports a wide range of applications, including ETL (extract, transform, load), Machine Learning, stream processing, and graph computation.\n",
    "- Also communicates well with some databases and other resources.\n",
    "- Installation of Spark and its dependencies is explained in the [Installation chapter](../../7_Appendix/Installation.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-17T10:34:51.972150Z",
     "start_time": "2024-08-17T10:34:51.966896Z"
    },
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Set environment variables for PySpark (system and version dependent!) \n",
    "# if not already set persistently (e.g., in .bashrc or .bash_profile or Windows environment variables)\n",
    "import os\n",
    "# Set the Java home path to the one you are using ((un)comment and edit as needed):\n",
    "# os.environ[\"JAVA_HOME\"] = \"C:/Program Files/Java/jre1.8.0_461\" # or similar on Windows (Liland's Windows, 461 is a moving target)\n",
    "# os.environ[\"JAVA_HOME\"] = \"/Library/Java/JavaVirtualMachines/zulu-18.jdk/Contents/Home\" (Liland's old Mac)\n",
    "# os.environ[\"JAVA_HOME\"] = \"/Library/Internet Plug-Ins/JavaAppletPlugin.plugin/Contents/Home\" # (Liland's Mac)\n",
    "os.environ[\"JAVA_HOME\"] = \"/Library/Java/JavaVirtualMachines/microsoft-17.jdk/Contents/Home\" # (Liland's Mac updated)\n",
    "\n",
    "# If you are using environments in Python, you can set the environment variables like the alternative below.\n",
    "# The default Python environment is used if the variables are set to \"python\" (edit if needed):\n",
    "os.environ[\"PYSPARK_PYTHON\"] = \"python\" # or similar to \"/Users/kristian/miniforge3/envs/tf_M1/bin/python\"\n",
    "os.environ[\"PYSPARK_DRIVER_PYTHON\"] = \"python\" # or similar to \"/Users/kristian/miniforge3/envs/tf_M1/bin/python\"\n",
    "\n",
    "# On Windows you need to specify where the Hadoop drivers are located (uncomment and edit if needed):\n",
    "# os.environ[\"HADOOP_HOME\"] = \"C:/NMBU/hadoop-3.3.1\" # (Liland's Windows)\n",
    "# os.environ[\"HADOOP_HOME\"] = \"C:/Hadoop/hadoop-3.3.1\"\n",
    "\n",
    "# Set the Hadoop version to the one you are using, e.g., none:\n",
    "os.environ[\"PYSPARK_HADOOP_VERSION\"] = \"without\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Spark and Cassandra\n",
    "- Cassandra is one of the databases that work well with Spark.\n",
    "    - Same type of distributed processing.\n",
    "    - Same way of replicating for fault tolerance.\n",
    "- Spark can be deployed on the same nodes as Cassandra for:\n",
    "    - local (short traveled) data manipulation, and\n",
    "    - combination of results to a central hub ([MapReduce](https://en.wikipedia.org/wiki/MapReduce)).\n",
    "- Requires drivers from Datastax\n",
    "    - Automatically downloaded and applied with the following configuration.\n",
    "    - The combination of Java version, Spark version and datax driver is very sensitive.\n",
    "- A [SparkSession](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.SparkSession.html) instantiates Spark, applies configurations and connects to a data source."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-17T10:35:04.311524Z",
     "start_time": "2024-08-17T10:34:53.919911Z"
    },
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName('SparkCassandraApp').\\\n",
    "    config('spark.jars.packages', 'com.datastax.spark:spark-cassandra-connector_2.12:3.5.1').\\\n",
    "    config('spark.cassandra.connection.host', 'localhost').\\\n",
    "    config('spark.sql.extensions', 'com.datastax.spark.connector.CassandraSparkExtensions').\\\n",
    "    config('spark.sql.catalog.mycatalog', 'com.datastax.spark.connector.datasource.CassandraCatalog').\\\n",
    "    config('spark.cassandra.connection.port', '9042').getOrCreate()\n",
    "# Some warnings are to be expected.\n",
    "# If running this cell does not give any output after ~30 seconds, there is likely an error in the configuration (JAVA_HOME, HADOOP_HOME, etc.)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Accessing tables\n",
    "**Note: The following sets of commands assume that the [Cassandra notebook](./3_Cassandra.ipynb) has been run first to set up the relevant keyspace and tables.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-17T10:25:51.439487Z",
     "start_time": "2024-08-17T10:25:51.439435Z"
    },
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# .load() is used to load data from Cassandra table as a Spark DataFrame.\n",
    "spark.read.format(\"org.apache.spark.sql.cassandra\").options(table=\"my_first_table\", keyspace=\"my_first_keyspace\").load().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Database views\n",
    "- Useful for \"setting the scene\" before a more simplified data extraction.\n",
    "- The below example simply attaches to the correct keyspace and table.\n",
    "    - The _view_ could also be a selection into that table to query further."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create view for simpler SQL queries\n",
    "spark.read.format(\"org.apache.spark.sql.cassandra\").options(table=\"table_with_uuid\", keyspace=\"my_first_keyspace\").load().createOrReplaceTempView(\"my_first_table_view\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### [Spark DataFrame](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrame.html)\n",
    "- Related to a Pandas data frame, but can be distributed over compute nodes.\n",
    "- Various functions like filters, statistical calculations, groupBy, Pandas functions (mapInPandas), joins, etc.\n",
    "- Export to Pandas and JSON.\n",
    "- Reads many formats, including SQL, JSON, Excel, ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Read CSV file into Spark DataFrame\n",
    "planets = spark.read.csv(\"../../data/planets.csv\", header=True, inferSchema=True)\n",
    "planets.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Select only Tesla company\n",
    "#             DataFrame                    -->\n",
    "spark.sql(\"select * from my_first_table_view\").filter(\"company = 'Tesla'\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Equivalent to the above but in pure SQL\n",
    "spark.sql(\"select * from my_first_table_view where company = 'Tesla'\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Select all data from the view and convert it to Pandas DataFrame\n",
    "spark.sql(\"select * from my_first_table_view\").toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# View data as a table and select only Tesla company\n",
    "df = spark.sql(\"select * from my_first_table_view\")\n",
    "df.filter(df.company == 'Tesla').toPandas() # Equivalent to \"company = 'Tesla'\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Filter also on price > 20000\n",
    "df.filter((df.company == 'Tesla') & (df.price > 20000)).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Aggregation, grouping and filtering\n",
    "- These can be combined in many ways.\n",
    "- Starting from the left.\n",
    "- Order is important."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Aggregate prices by company and sort by company name\n",
    "df.groupBy(\"company\").agg({\"price\": \"avg\"}).orderBy('company').toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Write data to Cassandra\n",
    "- One can append or overwrite data in existing database tables.\n",
    "- PySpark is picky regarding data formats.\n",
    "    - Reading data from the existing table and extracting formatting is possible.\n",
    "- PySpark is case sensitive, while Cassandra is not by default.\n",
    "    - See example of case sensitvity issues in the [Cassandra notebook](./3_Cassandra.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create two new cars in a Pandas DataFrame\n",
    "import pandas as pd\n",
    "newCars = pd.DataFrame([[459, 'Ford', 'Escort'], [460, 'Ford', 'Transit']], columns=['ind', 'company', 'model'])\n",
    "newCars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Convert the Pandas DataFrame to Spark DataFrame and save it to Cassandra (append mode)\n",
    "spark.createDataFrame(newCars).write.format(\"org.apache.spark.sql.cassandra\")\\\n",
    ".options(table=\"my_first_table\", keyspace=\"my_first_keyspace\").mode(\"append\").save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Check if the new cars are in the table\n",
    "spark.read.format(\"org.apache.spark.sql.cassandra\")\\\n",
    ".options(table=\"my_first_table\", keyspace=\"my_first_keyspace\").load()\\\n",
    ".createOrReplaceTempView(\"my_first_table_view2\")\n",
    "\n",
    "spark.sql(\"select * from my_first_table_view2\").toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Exercise\n",
    "- Create a table matching the structure of the planets data.\n",
    "- Insert the planets data into the table using Spark.\n",
    "- Read the data back from Cassandra through Spark and print it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Stop Spark session\n",
    "try:\n",
    "    spark.stop()\n",
    "except ConnectionRefusedError:\n",
    "    print(\"Spark session already stopped.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "```{seealso} \n",
    ":class: tip\n",
    "\n",
    "## Resources\n",
    "- [PySpark Tutorial For Beginners (sparkbyexample.com)](https://sparkbyexamples.com/pyspark-tutorial/)\n",
    "- [PySpark documentation](https://spark.apache.org/docs/latest/api/python/index.html)\n",
    "    - [PySpark DataFrame](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrame.html)\n",
    "    - [PySpark SparkSession](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.SparkSession.html)\n",
    "- [YouTube: PySpark Tutorial: Spark SQL & DataFrame Basics](https://youtu.be/3-pnWVWyH-s?si=5AfOao23gqgh19en) (17m:12s)\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "ind320_25",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
